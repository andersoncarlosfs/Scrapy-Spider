{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents web scraping tool that runs on top of [Scrapy](https://scrapy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To retrieve webpages that contain a given string.\n",
    "    1. To convert webpages into plain ASCII text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [WebDriver](https://www.w3.org/TR/webdriver1/)\n",
    "- [Crawler](https://en.wikipedia.org/wiki/Web_crawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "     from urlparse import urlparse\n",
    "import encodings       \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third-parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [scrapy](https://scrapy.org/)\n",
    "- [html2text](http://alir3z4.github.io/html2text/)\n",
    "- [selenium](https://www.seleniumhq.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "import html2text\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. List of URL from the websites to crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = 'websites.txt'\n",
    "city = 'vanves'\n",
    "activities = ''\n",
    "themes = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = []\n",
    "allowed_domains = []\n",
    "depth = 1\n",
    "tokens = []\n",
    "counter = 0\n",
    "charsets = set(encodings.aliases.aliases.values())\n",
    "driver = webdriver.Chrome()\n",
    "ignored_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowed domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preventing the spider to follow URLs not belonging to the allowed domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(websites) as i:        \n",
    "    for l in i:  \n",
    "        if not l.isspace():\n",
    "            parsed = urlparse(l.rstrip())\n",
    "            start_urls.append(parsed.geturl())\n",
    "            allowed_domains.append(parsed.netloc)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'exceptions.IOError'>\n",
      "(2, 'No such file or directory')\n",
      "[Errno 2] No such file or directory: ''\n",
      "<type 'exceptions.IOError'>\n",
      "(2, 'No such file or directory')\n",
      "[Errno 2] No such file or directory: ''\n"
     ]
    }
   ],
   "source": [
    "for f in [activities, themes]:\n",
    "    try:\n",
    "        with open(f) as i:      \n",
    "            for l in i:  \n",
    "                if not l.isspace():\n",
    "                    tokens.append(l)  \n",
    "    except Exception as e:           \n",
    "        print(type(e))   \n",
    "        print(e.args)\n",
    "        print(e)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that Scrapy will uses to scrape information from a single or a group of website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderScraper (scrapy.Spider):\n",
    "    '''    \n",
    "        The class that Scrapy will uses to scrape information from a single or a group of website.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: string, optional (default = query)\n",
    "            a string that defines the name for this spider.\n",
    "        start_urls: array, optional (default = allowed_domains) \n",
    "            an array of strings containing URLs where the spider will begin to crawl from.\n",
    "        allowed_domains: array, optional (default = allowed_domains)\n",
    "            an array of strings containing domains that this spider is allowed to crawl. \n",
    "        custom_settings: array, optional (default = ?)\n",
    "            ?\n",
    "        query: string, optional (default = query)\n",
    "            a string to be found in webpages.\n",
    "        counter: int, optional (default = counter)\n",
    "            an int that holds the number of retrieved webpages.\n",
    "        charsets: set, optional (default = charsets)   \n",
    "            a set of string containing the charsets that this spidel will use to decode webpages.\n",
    "        driver: ?\n",
    "            ?\n",
    "        ignored_urls: ?\n",
    "            ?\n",
    "        statics: ?\n",
    "            ?\n",
    "    '''\n",
    "    name = city\n",
    "    start_urls = start_urls    \n",
    "    allowed_domains = allowed_domains    \n",
    "    custom_settings = {\n",
    "        'DEPTH_LIMIT': depth,\n",
    "        'DEPTH_PRIORITY': 1,\n",
    "        'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue',\n",
    "        'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue'\n",
    "    }\n",
    "    query = city\n",
    "    parameters = tokens \n",
    "    counter = counter\n",
    "    charsets = charsets\n",
    "    driver = driver\n",
    "    ignored_urls = ignored_urls    \n",
    "    statistics = {\n",
    "        'time': {\n",
    "            'start': time.time(),\n",
    "            'end': None\n",
    "        },\n",
    "        'input': {\n",
    "            'urls': len(start_urls)            \n",
    "        },\n",
    "        'ouput': {\n",
    "            'documents': {\n",
    "                'positive': 0,\n",
    "                'negative': 0,\n",
    "                'uncertain': 0\n",
    "            }\n",
    "        }                    \n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        '''\n",
    "            The method that parses the response.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            response: ?\n",
    "                ?\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "            ?\n",
    "        '''\n",
    "        if not response.request.url in SpiderScraper.ignored_urls:              \n",
    "\n",
    "            driver.get(response.request.url)\n",
    "\n",
    "            html = driver.page_source\n",
    "\n",
    "            '''\n",
    "            for c in SpiderScraper.charsets:\n",
    "                try:                \n",
    "                    break\n",
    "                except UnicodeDecodeError as u:\n",
    "                    continue\n",
    "                except Exception as e: \n",
    "                    print response           \n",
    "                    print(type(e))   \n",
    "                    print(e.args)\n",
    "                    print(e)          \n",
    "            '''\n",
    "\n",
    "            '''\n",
    "                Converting the webpages into plain ASCII text.\n",
    "            '''\n",
    "            text = html2text.html2text(html)\n",
    "\n",
    "            '''\n",
    "                ?\n",
    "            '''\n",
    "            links = None\n",
    "\n",
    "            '''\n",
    "                Checking if the webpages contains the string to be found (query).\n",
    "            '''                \n",
    "            if SpiderScraper.query in text:                                            \n",
    "                if SpiderScraper.parameters:   \n",
    "                    for t in SpiderScraper.parameters:\n",
    "                        if t in text:\n",
    "                            links = scrapy.Selector(text = html, type = 'html').xpath('*//a/@href').extract()\n",
    "                            break\n",
    "                else:\n",
    "                    links = scrapy.Selector(text = html, type = 'html').xpath('*//a/@href').extract()\n",
    "\n",
    "            '''\n",
    "                ?\n",
    "            '''\n",
    "            if links is None:\n",
    "\n",
    "                SpiderScraper.ignored_urls.append(response.request.url)\n",
    "\n",
    "                '''\n",
    "                    Updating statistics\n",
    "                '''\n",
    "                statistics['ouput']['documents']['negative'] += 1 \n",
    "\n",
    "            else : \n",
    "\n",
    "                '''\n",
    "                    Updating statistics\n",
    "                '''\n",
    "                statistics['ouput']['documents']['positive'] += 1 \n",
    "\n",
    "                '''\n",
    "                    Persisting the webpages\n",
    "                '''\n",
    "                with open(str(SpiderScraper.counter) + '.txt', 'w') as f:            \n",
    "                            f.write(text.encode('utf-8'))\n",
    "\n",
    "                SpiderScraper.counter += 1\n",
    "\n",
    "                if response.meta['depth'] == SpiderScraper.custom_settings['DEPTH_LIMIT']:\n",
    "\n",
    "                    '''\n",
    "                        Updating statistics\n",
    "                    '''\n",
    "                    statistics['ouput']['documents']['uncertain'] += len(links)\n",
    "\n",
    "                    with open(str(SpiderScraper.counter - 1) + '_links.txt', 'w') as f:            \n",
    "                        for l in links:                \n",
    "                            f.write(\"%s\\n\" % l)\n",
    "                else:\n",
    "\n",
    "                    for l in links:\n",
    "                        yield response.follow(l, self.parse) \n",
    "                    \n",
    "        '''\n",
    "            Updating statistics\n",
    "        '''\n",
    "        statistics['time']['end'] = time.time()\n",
    "        \n",
    "        print statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This web scraping tool runs on top of Scrapy. In order to execute this notebook as a spider, it is necessary to convert this notebook into executable script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Breadth-first search (BFS) shifts between websites avoiding to explore non-compelling web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [BeautifulSoup Grab Visible Webpage Text](https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text)\n",
    "- [How can I render JavaScript HTML to HTML in python?](https://stackoverflow.com/questions/29404856/how-can-i-render-javascript-html-to-html-in-python)\n",
    "- [How to get html with javascript rendered sourcecode by using selenium](https://stackoverflow.com/questions/22739514/how-to-get-html-with-javascript-rendered-sourcecode-by-using-selenium)\n",
    "- [Rendered HTML to plain text using Python ](https://stackoverflow.com/questions/13337528/rendered-html-to-plain-text-using-python)\n",
    "- [Python - html2text write to file](https://stackoverflow.com/questions/28602868/python-html2text-write-to-file)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [Get protocol + host name from URL](https://stackoverflow.com/questions/9626535/get-protocol-host-name-from-url)\n",
    "- [How to extract raw html from a Scrapy selector?](https://stackoverflow.com/questions/34887730/how-to-extract-raw-html-from-a-scrapy-selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
