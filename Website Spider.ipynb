{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents web scraping tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To retrieve web pages that contain a given string.\n",
    "    1. To convert web pages into plain ASCII text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [WebDriver](https://www.w3.org/TR/webdriver1/)\n",
    "- [Crawler](https://en.wikipedia.org/wiki/Web_crawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "except ImportError:\n",
    "     from urlparse import urlparse\n",
    "import encodings       \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third-parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [scrapy](https://scrapy.org/)\n",
    "- [html2text](http://alir3z4.github.io/html2text/)\n",
    "- [selenium](https://www.seleniumhq.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "import html2text\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. List of URL from the websites to crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = 'websites.txt'\n",
    "query = 'vanves'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = []\n",
    "allowed_domains = []\n",
    "counter = 0\n",
    "charsets = set(encodings.aliases.aliases.values())\n",
    "driver = webdriver.Chrome()\n",
    "ignored_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowed domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preventing the spider to follow URLs not belonging to the allowed domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(websites) as i:        \n",
    "    for l in i:  \n",
    "        if not l.isspace():\n",
    "            parsed = urlparse(l.rstrip())\n",
    "            start_urls.append(parsed.geturl())\n",
    "            allowed_domains.append(parsed.netloc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that Scrapy will uses to scrape information from a single or a group of website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderScraper (scrapy.Spider):\n",
    "    '''    \n",
    "        The class that Scrapy will uses to scrape information from a single or a group of website.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: string, optional (default = query)\n",
    "            a string that defines the name for this spider.\n",
    "        start_urls: array, optional (default = allowed_domains) \n",
    "            an array of strings containing URLs where the spider will begin to crawl from.\n",
    "        allowed_domains: array, optional (default = allowed_domains)\n",
    "            an array of strings containing domains that this spider is allowed to crawl. \n",
    "        query: string, optional (default = query)\n",
    "            a string to be found in web pages.\n",
    "        counter: int, optional (default = counter)\n",
    "            an int that holds the number of retrieved web pages.\n",
    "        charsets: set, optional (default = charsets)   \n",
    "            a set of string containing the charsets that this spidel will use to decode web pages.\n",
    "        driver: ?\n",
    "            ?\n",
    "        ignored_urls: ?\n",
    "            ?\n",
    "    '''\n",
    "    name = query\n",
    "    start_urls = start_urls\n",
    "    allowed_domains = allowed_domains    \n",
    "    query = query \n",
    "    counter = counter\n",
    "    charsets = charsets\n",
    "    driver = driver\n",
    "    ignored_urls = ignored_urls\n",
    "    \n",
    "    def parse(self, response):\n",
    "        '''\n",
    "            The method that parses the response.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            response: ?\n",
    "                ?\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "            ?\n",
    "        '''\n",
    "        if response.request.url in SpiderScraper.ignored_urls:\n",
    "            return\n",
    "        \n",
    "        driver.get(response.request.url)\n",
    "        \n",
    "        html = driver.page_source\n",
    "          \n",
    "        '''\n",
    "        for c in SpiderScraper.charsets:\n",
    "            try:                \n",
    "                break\n",
    "            except UnicodeDecodeError as u:\n",
    "                continue\n",
    "            except Exception as e: \n",
    "                print response           \n",
    "                print(type(e))   \n",
    "                print(e.args)\n",
    "                print(e)          \n",
    "        '''\n",
    "\n",
    "        '''\n",
    "            Converting the web pages into plain ASCII text.\n",
    "        '''\n",
    "        text = html2text.html2text(html)\n",
    "        \n",
    "        '''\n",
    "            Persisting the web pages if they contains the string to be found (query).\n",
    "        '''\n",
    "        if SpiderScraper.query in text:\n",
    "            with open(str(SpiderScraper.counter) + '.txt', 'w') as f:            \n",
    "                f.write(text.encode('utf-8'))\n",
    "                SpiderScraper.counter += 1\n",
    "        else:\n",
    "            SpiderScraper.ignored_urls.append(response.request.url)\n",
    "        \n",
    "        '''\n",
    "            Searching for URLs using XPath.\n",
    "        '''\n",
    "        for link in scrapy.Selector(text = html, type = 'html').xpath('*//a/@href').extract():\n",
    "            yield response.follow(link, self.parse)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [BeautifulSoup Grab Visible Webpage Text](https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text)\n",
    "- [How can I render JavaScript HTML to HTML in python?](https://stackoverflow.com/questions/29404856/how-can-i-render-javascript-html-to-html-in-python)\n",
    "- [How to get html with javascript rendered sourcecode by using selenium](https://stackoverflow.com/questions/22739514/how-to-get-html-with-javascript-rendered-sourcecode-by-using-selenium)\n",
    "- [Rendered HTML to plain text using Python ](https://stackoverflow.com/questions/13337528/rendered-html-to-plain-text-using-python)\n",
    "- [Python - html2text write to file](https://stackoverflow.com/questions/28602868/python-html2text-write-to-file)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [Get protocol + host name from URL](https://stackoverflow.com/questions/9626535/get-protocol-host-name-from-url)\n",
    "- [How to extract raw html from a Scrapy selector?](https://stackoverflow.com/questions/34887730/how-to-extract-raw-html-from-a-scrapy-selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
